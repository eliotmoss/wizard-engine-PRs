// Really want to write something like this, Virgil does not support it:
//
// class LayoutRange<Ref<L>> {
//	def var rng: Range<byte>;
//	new(rng);
//	def [index: u64] => Ref<L>.at(rng[index * L.size ..+ L.size]);
// }

// Layout ranges may go into a Virgil library once tested and refined

// The concept is essentially an array/range of a layout, i.e., multiple
// copies of the same layout up against each other.  The "at" function does
// the actual indexing and constructs a Ref, so this class does little actual
// work (but see the companion component, LayoutRanges).

class LayoutRange<T> {
	def var rng: Range<byte>;  // the underlying range of bytes
	def at: (Range<byte>, u64) -> T;  // a function to index that range and return a Ref
	def offset: u64 -> u64;  // a function to go from index to offset
	def [index: u64] -> T { return at(rng, index); }  // indexing that supports [i] notation
	new(rng, at, offset) { }
}

// This component provides several functions that can be used to index a
// LayoutRange with different semantics for the index passed in.

component LayoutRanges {

	// atIndex interprets the index as indicating which element of the
	// range is desired.  Thus, an index of 2 indicates the third element
	// of the range.  atIndex requires the size in order to calculate the
	// byte offset.
	def atIndex<T>(rng: Range<byte>, index: u64, size: int, at: Range<byte> -> T) -> T {
		return at(rng[index * u64.!(size) ..+ size]);
	}

	// atByte interprets the index as indicating the starting byte.  Thus, an index
	// of 2 indicates to start at the third *byte* on the range.  It uses the size
	// only to determine the size of the subrange passed to "at".
	def atByte<T>(rng: Range<byte>, pos: u64, size: int, at: Range<byte> -> T) -> T {
		return at(rng[pos ..+ size]);
	}

	// forOffset takes an offset (into some other range of bytes) and
	// converts that into a suitable index in this LayoutRange based on
	// their being one layout per some number of bytes in the other range.
	// Here an offset of 2 means to convert from that offset in the
	// original range.  To perform the computation, forOffset requires a
	// "divisor" (the number of bytes in the other range per layout in
	// this range) and the size of each layout in this range.
	def forOffset<T>(rng: Range<byte>, offset: u64, divisor: u64, size: int,
			at: Range<byte> -> T) -> T {
		return at(rng[(offset / divisor) * u64.!(size) ..+ size]);
	}

	private def offset(base: u64, size: int, index: u64) -> u64 {
		return base + index * u64.!(size);
	}
	// Creates a LayoutRange that uses the atIndex accessor and gives offset as within the original range
	// Example: LayoutRanges.subrangeByIndex(rng, start, L.size, count, Ref<L>.at)
	def subrangeByIndex<T>(rng: Range<byte>, base: u64, size: int, count: u64, atFunc: Range<byte> -> T) -> LayoutRange<T> {
		var sub = rng[base ..+ count * u64.!(size)];
		return LayoutRange<T>.new(sub, LayoutRanges.atIndex(_, _, size, atFunc), offset(base, size, _));
	}

	// Creates a LayoutRange that uses the atIndex accessor
	// Example: LayoutRanges.byIndex(rng, L.size, Ref<L>.at)
	def byIndex<T>(rng: Range<byte>, size: int, atFunc: Range<byte> -> T) -> LayoutRange<T> {
		return LayoutRange<T>.new(rng, LayoutRanges.atIndex(_, _, size, atFunc), u64.*(u64.!(size),_));
	}

	// Creates a LayoutRange that uses the atByte accessor
	// Example: LayoutRanges.byByte(rng, L.size, Ref<L>.at)
	def byByte<T>(rng: Range<byte>, size: int, atFunc: Range<byte> -> T) -> LayoutRange<T> {
		return LayoutRange<T>.new(rng, LayoutRanges.atByte(_, _, size, atFunc), u64.*(u64.!(size),_));
	}

	// Creates a LayoutRange that uses the forOffset accessor
	// Example: LayoutRanges.byOffset(rng, otherUnit, L.size, Ref<L>.at)
	def byOffset<T>(rng: Range<byte>, divisor: u64, size: int, atFunc: Range<byte> -> T) -> LayoutRange<T> {
		return LayoutRange<T>.new(rng, LayoutRanges.forOffset(_, _, divisor, size, atFunc), u64.*(u64.!(size),_));
	}
}

layout PWRegionHeader {
	+0	blockTable:	u64;	// relative offset to the block table
	+8	sentinels:	u64;	// offset of the sentinels (whatever type are!)
	+16	numBlocks:	u64;
	+24	numBytes:	u64;
	+32	numDescs:	u64;
	+40	metaData:	u64;	// relative offset to the metadata area
	+48	metaDataDescs:	u64;	// relative offset to the metadata descriptors
	=56;				// size of header
}

type BlockTableHandle(addr: Pointer, size: u64) #unboxed {
	def [n: i64] -> BlockEntryHandle {
		return if(
			u64.view(n) < size, 
			BlockEntryHandle(addr + n * BlockEntry.size), 
			BlockEntryHandle(Pointer.NULL));
	}
}

layout MetaDataDesc {
	+0	kind:		u64;  // unique code, identifying the kind of metadata
	+8	fixedBytes:	u32;  // single-copy data at beginning of metadata block
	+12	unitSize:	u32;  // number of bytes of data space per metadata item
	+16	bytesPerUnit:	u32;  // size of each metadata item, in bytes
				      // gap
	+24	offset:		u64;  // offset in region of the actual metadata
	=32;
}

type BlockEntryHandle(addr: Pointer) #unboxed {
	def isNull() -> bool {
		return addr == Pointer.NULL;
	}

	def toHandle(offset: i64) -> BlockEntryHandle {
		// Never need to point to ourselves
		return BlockEntryHandle(addr + offset);
	} 

	def toOffset(handle: BlockEntryHandle) -> i64 {
		return handle.addr - this.addr;
	}

	def getRef() -> Ref<BlockEntry> {
		return Ref<BlockEntry>.of(CiRuntime.forgeRange<byte>(addr, BlockEntry.size));
	}

	def getListNum() -> i16 {
		return getRef().listNum;
	}

	def getUsed() -> bool {
		return getRef().used == 1u1;
	}

	def getPrev() -> BlockEntryHandle {
		return toHandle(getRef().prev);
	}

	def getNext() -> BlockEntryHandle {
		return toHandle(getRef().next);
	}

	def getListPrev() -> BlockEntryHandle {
		return toHandle(getRef().listPrev);
	}

	def getListNext() -> BlockEntryHandle {
		return toHandle(getRef().listNext);
	}

	def getListNumCached(cache: WALCache) -> i16 {
		return cache.readI16(Pointer.atField(getRef().listNum));
	}

	def getUsedCached(cache:WALCache) -> bool {
		def used = cache.readU8(Pointer.atField(getRef().used));
		return used == 1u1;
	}

	def getPrevCached(cache: WALCache) -> BlockEntryHandle {
		return toHandle(cache.readI64(Pointer.atField(getRef().prev)));
	}

	def getNextCached(cache: WALCache) -> BlockEntryHandle {
		return toHandle(cache.readI64(Pointer.atField(getRef().next)));
	}

	def getListPrevCached(cache: WALCache) -> BlockEntryHandle {
		return toHandle(cache.readI64(Pointer.atField(getRef().listPrev)));
	}

	def getListNextCached(cache: WALCache) -> BlockEntryHandle {
		return toHandle(cache.readI64(Pointer.atField(getRef().listNext)));
	}

	def setListNum(listNum: i16) {
		getRef().listNum = listNum;
	}

	def setUsed(used: bool) {
		getRef().used = if(used, 1u1, 0u1);
	}

	def setPrev(prev: BlockEntryHandle) {
		getRef().prev = toOffset(prev);
	}

	def setNext(next: BlockEntryHandle) {
		getRef().next = toOffset(next);
	}

	def setListPrev(listPrev: BlockEntryHandle) {
		getRef().listPrev = toOffset(listPrev);
	}

	def setListNext(listNext: BlockEntryHandle) {
		getRef().listNext = toOffset(listNext);
	}

	def setListNumCached(cache: WALCache, listNum: i16) {
		cache.writeI16(Pointer.atField(getRef().listNum), listNum);
	}

	def setUsedCached(cache: WALCache, used: bool) {
		def value = if(used, 1u1, 0u1);
		cache.writeU8(Pointer.atField(getRef().used), value);
	}

	def setPrevCached(cache: WALCache, prev: BlockEntryHandle) {
		cache.writeI64(Pointer.atField(getRef().prev), toOffset(prev));
	}

	def setNextCached(cache: WALCache, next: BlockEntryHandle) {
		cache.writeI64(Pointer.atField(getRef().next), toOffset(next));
	}

	def setListPrevCached(cache: WALCache, listPrev: BlockEntryHandle) {
		cache.writeI64(Pointer.atField(getRef().listPrev), toOffset(listPrev));
	}

	def setListNextCached(cache: WALCache, listNext: BlockEntryHandle) {
		cache.writeI64(Pointer.atField(getRef().listNext), toOffset(listNext));
	}
}

def getBlockEntryHandle(r: Ref<BlockEntry>) -> BlockEntryHandle {
	return BlockEntryHandle(Pointer.atRef(r));
}

type ChunkHandle(addr: Pointer) #unboxed {
	def isNull() -> bool {
		return addr == Pointer.NULL;
	}

	def getRef() -> Ref<ChunkHeader> {
		return Ref<ChunkHeader>.of(CiRuntime.forgeRange<byte>(addr, ChunkHeader.size));
	}

	def getOffset() -> u64 {
		return getRef().offset;
	}

	def getLimit() -> u64 {
		return getRef().limit;
	}

	def getChunkLimit() -> u64 {
		return getRef().chunk_limit;
	}

	def getLinemarks() -> u64 {
		return getRef().linemarks;
	}

	def setOffset(v: u64) {
		getRef().offset = v;
	}

	def setLimit(v: u64) {
		getRef().limit = v;
	}

	def setChunkLimit(v: u64) {
		getRef().chunk_limit = v;
	}

	def setLinemarks(v: u64) {
		getRef().linemarks = v;
	}

	def setOffsetCached(cache: WALCache, v: u64) {
		cache.writeU64(Pointer.atField(getRef().offset), v);
	}

	def setLimitCached(cache: WALCache, v: u64) {
		cache.writeU64(Pointer.atField(getRef().offset), v);
	}
}

layout BlockEntry {
	+0	listNum:	i16;  // which list is this entry part of
	+2	used:		u1;   // is this block in use? // TODO: Use bool after the compiler is fixed
				      // gap
	+8	prev:		i64;  // prev entry in memory;
	+16	next:		i64;  // next entry in memory order
	+24	listPrev:	i64;  // prev entry on same list
	+32	listNext:	i64;  // next entry on same list
	=40;			      // size of block entry
}

layout ChunkHeader {
	+0	offset:		u64; // offset in chunk for next allocation
	+8	limit:		u64; // offset just beyond last available byte in chunk
	+16	chunk_limit:	u64; // offset just beyond end of the chunk
	+24	linemarks:	u64; // offset to first link mark of the chunk
	=32;			     // size of chunk header
}

enum ListKind(code: i16) {
	METADATA(-1),
	NONE(-2),
	USED(-3),
	SMALL_FREE(0),
	LARGE_FREE(1)
}

component BlockLists {
	def MAX_LISTS = 8;	// maximum number of lists == number of list sentinels
				// METADATA, etc., do not have sentinels and not part of this count
}

type LogEntry(addr: Pointer) { }

// Generic block-based region manager (platform-independent)
class PWRegion {
	def var backendRegion: BackendRegion;
	def var rng: Range<byte>;  // the actual bytes of the region
	def numBlocks: u64;
	def blockSize: u64;  // Size of each block
	def metaDataDescs: Range<Ref<MetaDataDesc>>;
	def var descs: LayoutRange<Ref<MetaDataDesc>>;
	def var sentinels: BlockTableHandle;
	def var smallSentinel: BlockEntryHandle;
	def var largeSentinel: BlockEntryHandle;
	def blankChunkHeader = Array<byte>.new(ChunkHeader.size);  // so we can return *something* when allocation fails
	def blankChunkHandle = ChunkHandle(Pointer.atElement(blankChunkHeader, 0));
	def var blockTableHandle: BlockTableHandle;
	var blockLayouts: LayoutRange<Ref<BlockEntry>>;
	def var logChunk: ChunkHandle;
	def var logPointer: LogEntry;
	def var walCache = WALCache.new();

	new(backend: TxnRegionBackend, numBlocks, metaDataDescs, blockSize: u64) {
		var size = numBlocks * blockSize;
		var prot = BackendProt.READ | BackendProt.WRITE;
		backendRegion = backend.allocate(size, prot);
		if (backendRegion == null) {
			// Allocation failed
			return;
		}
		rng = backendRegion.range;
		init(blockSize);
	}
	
	private def sanitizeOffset(offset: u64) {
		if (PWRegions.sanitize) {
			var addr: long = Pointer.atElement(rng, 0) - Pointer.NULL;
			if (offset < rng.length) {
				Trace.OUT.put3("offset %d is good for region at 0x%x of length %d", long.view(offset), addr, rng.length).ln();
			} else {
				Trace.OUT.put3("offset %d is bad for region at 0x%x of length %d", long.view(offset), addr, rng.length).ln();
			}
		}
	}
	
	private def sanitizeAddress(addr: Pointer) {
		if (PWRegions.sanitize) {
			var delta: long = addr - Pointer.atElement(rng, 0);
			var addrAsLong: long = addr - Pointer.NULL;
			var baseAddrAsLong: long = Pointer.atElement(rng, 0) - Pointer.NULL;
			if (delta < 0 || delta >= rng.length) {
				Trace.OUT.put3("address 0x%x is bad for region at 0x%x of length %d", addrAsLong, baseAddrAsLong, rng.length).ln();
			} else {
				Trace.OUT.put3("address 0x%x is good for region at 0x%x of length %d", addrAsLong, baseAddrAsLong, rng.length).ln();
			}
		}
	}
	
	def offsetInRegion(addr: u64) -> u64 {
		return addr - u64.view((Pointer.atElement(rng, 0) - Pointer.NULL));
	}

	def blockOfOffset(offset: u64) -> u64 {
		return offset / blockSize;
	}

	def blockIndexOf(bte: BlockEntryHandle) -> u64 {
		return u64.view((bte.addr - blockTableHandle.addr) / BlockEntry.size);
	}

	def chunkForBlockEntry(blk: BlockEntryHandle) -> ChunkHandle {
		return ChunkHandle(Pointer.atElement(rng, 0) + long.view(blockSize * blockIndexOf(blk)));
	}

	private def init(blockSize: u64) {
		def nullBTE = BlockEntryHandle(Pointer.NULL);
		var size = numBlocks * blockSize;
		var hdr = getHeader();
		sanitizeAddress(Pointer.atRef(hdr));
		hdr.numBlocks = this.numBlocks;
		hdr.numBytes = size;
		var currentRegionEnd = size;

		// set up list sentinels just after the header
		hdr.sentinels = u64.!(PWRegionHeader.size);
		sentinels = BlockTableHandle(Pointer.atElement(rng, int.!(hdr.sentinels)), u64.!(BlockLists.MAX_LISTS));
		sanitizeOffset(hdr.sentinels);
		sanitizeOffset(hdr.sentinels + u64.!(sentinels.size));
		for (i < BlockLists.MAX_LISTS) {
			var entryHandle = sentinels[i];
			entryHandle.setListNum(0);
			entryHandle.setUsed(false);
			entryHandle.setPrev(nullBTE);
			entryHandle.setNext(nullBTE);
			entryHandle.setListPrev(nullBTE);
			entryHandle.setListNext(nullBTE);
		}

		// take space for block table, + 1 for the marker block at the end
		var numBlockTableEntries = numBlocks + 1;
		var blockTableSize = numBlockTableEntries * u64.!(BlockEntry.size);
		currentRegionEnd -= blockTableSize;
		hdr.blockTable = currentRegionEnd;
		sanitizeOffset(currentRegionEnd);
		blockTableHandle = BlockTableHandle(Pointer.atElement(rng, int.!(currentRegionEnd)), numBlockTableEntries);
		
		// Initialize blockLayouts as a LayoutRange
		blockLayouts = LayoutRanges.byIndex<Ref<BlockEntry>>(
			rng[currentRegionEnd ..+ blockTableSize], BlockEntry.size, Ref<BlockEntry>.of);

		// clear block table entries (filled in better later)
		for (i < numBlockTableEntries) {
			var entryHandle = blockTableHandle[i64.!(i)];
			entryHandle.setListNum(0);
			entryHandle.setUsed(false);
			entryHandle.setPrev(nullBTE);
			entryHandle.setNext(nullBTE);
			entryHandle.setListPrev(nullBTE);
			entryHandle.setListNext(nullBTE);
		}

		// take space for metadata descriptors
		var metaDataDescsSize = metaDataDescs.length * MetaDataDesc.size;
		currentRegionEnd -= u64.!(metaDataDescsSize);
		sanitizeOffset(currentRegionEnd);
		hdr.metaDataDescs = currentRegionEnd;
		hdr.numDescs = u64.!(metaDataDescs.length);
		var metaDataLayouts = LayoutRanges.byIndex<Ref<MetaDataDesc>>(
					rng[hdr.metaDataDescs ..+ metaDataDescsSize], MetaDataDesc.size, Ref<MetaDataDesc>.of);

		// fill metadata descriptors and allocate metadata space
		for (i < metaDataDescs.length) {
			var desc = metaDataDescs[i];
			var metaDataSize = u64.!(desc.fixedBytes);
			var numUnits = (size + desc.unitSize - 1) / desc.unitSize;
			metaDataSize += numUnits * desc.bytesPerUnit;
			currentRegionEnd -= metaDataSize;
			sanitizeOffset(currentRegionEnd);
			var newDesc = metaDataLayouts[u64.!(i)];
			newDesc.kind = desc.kind;
			newDesc.fixedBytes = desc.fixedBytes;
			newDesc.unitSize = desc.unitSize;
			newDesc.bytesPerUnit = desc.bytesPerUnit;
			newDesc.offset = currentRegionEnd;
		}

		var metaDataBytes = u64.!(rng.length) - currentRegionEnd;
		var metaDataBlocks = (metaDataBytes + (blockSize - 1)) / blockSize;
		var userDataBlocks = numBlocks - (metaDataBlocks + 1);
		

		// set up block lists
		var firstRef = blockLayouts[0];
		var firstHandle = getBlockEntryHandle(firstRef);

		var secondRef = blockLayouts[1];
		var secondHandle = getBlockEntryHandle(secondRef);
	
		var thirdRef = blockLayouts[2];
		var thirdHandle = getBlockEntryHandle(thirdRef);

		smallSentinel = sentinels[i64.!(ListKind.SMALL_FREE.code)];

		largeSentinel = sentinels[i64.!(ListKind.LARGE_FREE.code)];

		var metaRef = blockLayouts[userDataBlocks + 1];
		var metaHandle = getBlockEntryHandle(metaRef);

		var lastRef = blockLayouts[numBlocks];
		var lastHandle = getBlockEntryHandle(lastRef);

		sanitizeAddress(firstHandle.addr);
		firstHandle.setListNum(ListKind.METADATA.code);
		firstHandle.setUsed(true);
		firstHandle.setNext(secondHandle);
		sanitizeOffset(blockLayouts.offset(1));

		sanitizeAddress(secondHandle.addr);  // log block
		secondHandle.setListNum(ListKind.METADATA.code);
		secondHandle.setUsed(true);
		secondHandle.setNext(thirdHandle);
		sanitizeOffset(blockLayouts.offset(2));
		logChunk = chunkForBlockEntry(secondHandle);

		sanitizeAddress(thirdHandle.addr);
		thirdHandle.setListNum(ListKind.LARGE_FREE.code);
		thirdHandle.setNext(metaHandle);
		thirdHandle.setPrev(firstHandle);
		thirdHandle.setListPrev(largeSentinel);
		thirdHandle.setListNext(metaHandle);
		sanitizeOffset(blockLayouts.offset(userDataBlocks + 1));
		sanitizeOffset(blockLayouts.offset(0));
		sanitizeOffset(hdr.sentinels + u64.!(ListKind.LARGE_FREE.code * i64.!(BlockEntry.size)));

		sanitizeAddress(smallSentinel.addr);
		// smallSentinel starts empty - points to null
		smallSentinel.setListNext(nullBTE);
		sanitizeOffset(blockLayouts.offset(0));

		sanitizeAddress(largeSentinel.addr);

		largeSentinel.setListNext(secondHandle);
		largeSentinel.setListPrev(metaHandle);
		sanitizeOffset(blockLayouts.offset(1));
		
		sanitizeAddress(metaHandle.addr);
		metaHandle.setListNum(ListKind.METADATA.code);
		metaHandle.setUsed(true);
		metaHandle.setNext(lastHandle);
		metaHandle.setPrev(secondHandle);
		metaHandle.setListPrev(largeSentinel);
		metaHandle.setListNext(nullBTE);
		sanitizeOffset(blockLayouts.offset(numBlocks));
		sanitizeOffset(blockLayouts.offset(1));
		
		sanitizeAddress(lastHandle.addr);
		lastHandle.setListNum(ListKind.NONE.code);
		lastHandle.setUsed(true);
		lastHandle.setPrev(metaHandle);
		sanitizeOffset(blockLayouts.offset(userDataBlocks + 1));
		
		// Initialize descs for subclasses that need to access metadata descriptors
		descs = metadataDescs();
	}
	
	def getHeader() -> Ref<PWRegionHeader> {
		return Ref<PWRegionHeader>.of(rng[0 ..+ PWRegionHeader.size]);
	}
	
	def regionHeaderSize() -> int {
		return PWRegionHeader.size;
	}
	
	def getBlockSize() -> u64 {
		return blockSize;
	}
	
	def metadataDescs() -> LayoutRange<Ref<MetaDataDesc>> {
		var hdr = getHeader();
		var metaDataDescsSize = hdr.numDescs * u64.!(MetaDataDesc.size);
		return LayoutRanges.byIndex<Ref<MetaDataDesc>>(
					rng[hdr.metaDataDescs ..+ metaDataDescsSize], MetaDataDesc.size, Ref<MetaDataDesc>.of);
	}
	
	def blockIndexOfEntry(entry: BlockEntryHandle) -> u64 {
		var hdr = getHeader();
		var blkTablePtr = Pointer.atElement(rng, int.!(hdr.blockTable));
		var entryPtr = entry.addr;
		var index = u64.!((entryPtr - blkTablePtr) / BlockEntry.size);
		return index;
	}
	
	def blockOffsetOfIndex(index: u64) -> u64 {
		return index * getBlockSize();
	}

	def createChunk(startEntry: BlockEntryHandle, numBlocks: u32) -> ChunkHandle {
		var blockIndex = blockIndexOfEntry(startEntry);
		var blockOffset = blockOffsetOfIndex(blockIndex);

	        var ptr = Pointer.atElement(rng, int.!(blockOffset));
	        var chunk = ChunkHandle(ptr);

		chunk.setOffsetCached(walCache, u64.!(ChunkHeader.size));
                // LOG: Chunk Header Initialization
                // Address: chunk.addr + ChunkHeader.offset (offset 0)
                // Size: 8 bytes
                // New Value: ChunkHeader.size

		chunk.setLimitCached(walCache, numBlocks * getBlockSize());
                // LOG: Chunk Header Limit
                // Address: chunk.addr + ChunkHeader.limit (offset 8)
                // Size: 8 bytes
                // New Value: numBlocks * getBlockSize()

		// TODO link mark field
		return chunk;
	}
	
	def allocChunk(numBlocks: u32) -> ChunkHandle {
		if (numBlocks == 1) {
			// Try the small free block list
			var firstHandle = smallSentinel.getListNext();
			if (!firstHandle.isNull()) {
                                // Note: removeFromList handles its own logging
				removeFromList(firstHandle);

                                // LOG: BlockEntry Used Flag
                                // Address: firstHandle.addr + BlockEntry.used (offset 2)
                                // Size: 1 byte
                                // New Value: 1 (true)
				firstHandle.setUsed(true);

                                // Note: createChunk handles logging of header initialization
				return createChunk(firstHandle, 1);
			}
		}
		var thisHandle = largeSentinel.getListNext();
		while (!thisHandle.isNull()) {
			var thisIndex = blockIndexOfEntry(thisHandle);
			var nextHandle = thisHandle.getNext();
			if (nextHandle.isNull()) return blankChunkHandle;
			var nextIndex = blockIndexOfEntry(nextHandle);
			var chunkBlocks = nextIndex - thisIndex;
			if (numBlocks > chunkBlocks) {
				thisHandle = thisHandle.getNext();
				continue;
			}
			if (numBlocks == chunkBlocks) {
                                // Note: removeFromList handles its own logging
				removeFromList(thisHandle);

                                // LOG: BlockEntry Used Flag
                                // Address: thisHandle.addr + BlockEntry.used (offset 2)
                                // Size: 1 byte
                                // New Value: 1 (true)
				thisHandle.setUsed(true);

                                // Note: createChunk handles logging of header initialization
				return createChunk(thisHandle, numBlocks);
			}

                        // --- Splitting Block Logic ---

			var remIndex = thisIndex + u64.!(numBlocks);
			var remRef = blockLayouts[remIndex];
			var remHandle = getBlockEntryHandle(remRef);
			var remBlocks = nextIndex - remIndex;

                        // LOG: Remainder Block Metadata Initialization
                        // 1. ListNum (Offset 0, 2 bytes): SMALL_FREE or LARGE_FREE
                        // 2. Used (Offset 2, 1 byte): 0 (false)
                        // 3. Prev (Offset 8, 8 bytes): remHandle.toOffset(thisHandle)
                        // 4. Next (Offset 8, 8 bytes): remHandle.toOffset(nextHandle)
                        // 5. ListPrev (Offset 24, 8 bytes): remHandle.toOffset(thisHandle)
                        // 6. ListNext (Offset 32, 8 bytes): remHandle.toOffset(nextHandle)
			remHandle.setListNum(if(remBlocks == 1, ListKind.SMALL_FREE.code, ListKind.LARGE_FREE.code));
			remHandle.setUsed(false);
			remHandle.setPrev(thisHandle);
			remHandle.setNext(nextHandle);
			remHandle.setListPrev(thisHandle);
			remHandle.setListNext(nextHandle);

                        // LOG: Update 'this' block Next pointer
                        // Address: thisHandle.addr + BlockEntry.next (offset 16)
                        // Size: 8 bytes
                        // New Value: thisHandle.toOffset(remHandle)
			thisHandle.setNext(remHandle);

                        // LOG: Update 'this' block ListNext pointer
                        // Address: thisHandle.addr + BlockEntry.listNext (offset 32)
                        // Size: 8 bytes
                        // New Value: thisHandle.toOffset(remHandle)
			thisHandle.setListNext(remHandle);

                        // LOG: Update 'next' block Prev pointer
                        // Address: nextHandle.addr + BlockEntry.prev (offset 8)
                        // Size: 8 bytes
                        // New Value: nextHandle.toOffset(remHandle)
			nextHandle.setPrev(remHandle);

			var listNextHandle = thisHandle.getListNext();
                        
                        // LOG: Update 'listNext' block ListPrev pointer
                        // Address: listNextHandle.addr + BlockEntry.listPrev (offset 24)
                        // Size: 8 bytes
                        // New Value: listNextHandle.toOffset(remHandle)
			listNextHandle.setListPrev(remHandle);

                        // Note: removeFromList handles its own logging
			removeFromList(thisHandle);

			return createChunk(thisHandle, numBlocks);
		}
		return blankChunkHandle;
	}

	def freeChunk(chunk: ChunkHandle) {
		if (chunk.isNull()) return;

		// Map the chunk address back to the BlockEntry
		def index = blockOfOffset(offsetInRegion(u64.view(chunk.addr - Pointer.NULL)));

		// Retrieve the handle for the header of this block
		var entry = blockTableHandle[long.view(index)];

		// Ensure we aren't freeing an already free block
		if (!entry.getUsed()) return;

		entry.setUsedCached(walCache, false);
                // LOG: Mark Block as Unused
                // Address: entry.addr + BlockEntry.used (offset 2)
                // Size: 1 bytes
                // New Value: 0 (false)

		// Coalesce Left
		def prev = entry.getPrevCached(walCache);
		if (!prev.isNull() && !prev.getUsedCached(walCache)) {
                        // Note: removeFromList handles its own logging
			removeFromList(prev); // Remove prev from its specific free list

			def next = entry.getNextCached(walCache);

			prev.setNextCached(walCache, next);
                        // LOG: Update Prev's Next pointer (Coalescing)
                        // Address: prev.addr + BlockEntry.next (offset 16)
                        // Size: 8 bytes
                        // New Value: prev.toOffset(next)

			if (!next.isNull()) {
				next.setPrevCached(walCache, prev);
                                // LOG: Update Next's Prev pointer (Coalescing)
                                // Address: next.addr + BlockEntry.prev (offset 8)
                                // Size: 8 bytes
                                // New Value: next.toOffset(prev)
			}
			// Update local handle to point to 'prev' so subsequent logic works on the merged head.
			entry = prev;
		}

		// Coalesce Right
		def next = entry.getNextCached(walCache);
		if (!next.isNull() && !next.getUsedCached(walCache)) {
                        // Note: removeFromList handles its own logging
			removeFromList(next); // Remove next from its specific free list temporarily

			def nextNext = next.getNextCached(walCache);

			entry.setNextCached(walCache, nextNext);
                        // LOG: Update Entry's Next pointer (Coalescing)
                        // Address: entry.addr + BlockEntry.next (offset 16)
                        // Size: 8 bytes
                        // New Value: entry.toOffset(nextNext)

			if (!nextNext.isNull()) {
				nextNext.setPrevCached(walCache, entry);
                                // LOG: Update NextNext's Prev pointer (Coalescing)
                                // Address: nextNext.addr + BlockEntry.prev (offset 8)
                                // Size: 8 bytes
                                // New Value: nextNext.toOffset(entry)
			}
		}

		// Calculate new size to determine which list to use
		def startIndex = blockIndexOfEntry(entry);
		def nextNeighbor = entry.getNext();
		// If there is no next neighbor, the block extends to the end of the region (numBlocks)
		def endIndex = if (nextNeighbor.isNull(), numBlocks, blockIndexOfEntry(nextNeighbor));
		def newSize = endIndex - startIndex;

                // Note: insertAfter handles its own logging
		if (newSize == 1) {
			insertAfter(smallSentinel, entry, ListKind.SMALL_FREE);
		} else {
			insertAfter(largeSentinel, entry, ListKind.LARGE_FREE);
		}
	}

	// Helper to insert an entry into a list immediately after the sentinel (Head insertion)
	def insertAfter(sentinel: BlockEntryHandle, entry: BlockEntryHandle, kind: ListKind) {
		entry.setListNumCached(walCache, kind.code);
                // LOG: Set List Kind
                // Address: entry.addr + BlockEntry.listNum (offset 0)
                // Size: 2 bytes
                // New Value: kind.code
	
		var listNext = sentinel.getListNextCached(walCache);
	
		// Link: sentinel -> entry -> listNext

		sentinel.setListNextCached(walCache, entry);
                // LOG: Link Sentinel to Entry
                // Address: sentinel.addr + BlockEntry.listNext (offset 32)
                // Size: 8 bytes
                // New Value: sentinel.toOffset(entry)
                
		entry.setListPrevCached(walCache, sentinel);
                // LOG: Link Entry to Sentinel (Prev)
                // Address: entry.addr + BlockEntry.listPrev (offset 24)
                // Size: 8 bytes
                // New Value: entry.toOffset(sentinel)
        
	  	entry.setListNextCached(walCache, listNext);
                // LOG: Link Entry to listNext (Next)
                // Address: entry.addr + BlockEntry.listNext (offset 32)
                // Size: 8 bytes
                // New Value: entry.toOffset(listNext)
	
		if (!listNext.isNull()) {
			listNext.setListPrevCached(walCache, entry);
                        // LOG: Link listNext back to Entry (Prev)
                        // Address: listNext.addr + BlockEntry.listPrev (offset 24)
                        // Size: 8 bytes
                        // New Value: listNext.toOffset(entry)
		}
	}

	def blockEntryAt(offset: u64) -> Ref<BlockEntry> {
		return Ref<BlockEntry>.of(rng[offset ..+ BlockEntry.size]);
	}
	
	def removeFromList(blk: BlockEntryHandle) {
		var listPrev = blk.getListPrevCached(walCache);
		var listNext = blk.getListNextCached(walCache);

		if (!listPrev.isNull()) {
			listPrev.setListNextCached(walCache, listNext);
                        // LOG: Update listPrev's ListNext
                        // Address: listPrev.addr + BlockEntry.listNext (offset 32)
                        // Size: 8 bytes
                        // New Value: listNextOffsetFromListPrev 
			// listPrev.setListNext(listNext);
		}

		if (!listNext.isNull()) {
			listNext.setListPrevCached(walCache, listPrev);
                        // LOG: Update listNext's ListPrev
                        // Address: listNext.addr + BlockEntry.listPrev (offset 24)
                        // Size: 8 bytes
                        // New Value: listPrevOffsetFromListNext
		}

		def nullBTE = BlockEntryHandle(Pointer.NULL);
                
		blk.setListPrevCached(walCache, nullBTE);
                // LOG: Clear blk ListPrev
                // Address: blk.addr + BlockEntry.listPrev (offset 24)
                // Size: 8 bytes
                // New Value: null (0)

		blk.setListNextCached(walCache, nullBTE);
                // LOG: Clear blk ListNext
                // Address: blk.addr + BlockEntry.listNext (offset 32)
                // Size: 8 bytes
                // New Value: null (0)

		blk.setListNumCached(walCache, ListKind.NONE.code);
                // LOG: Clear blk ListNum
                // Address: blk.addr + BlockEntry.listNum (offset 0)
                // Size: 2 bytes
                // New Value: ListKind.NONE.code (-2)
	}
	
	// Deallocate this region
	def deallocate() {
		if (backendRegion != null) {
			backendRegion.backend.deallocate(backendRegion);
			backendRegion = null;
		}
	}
}

layout LogEntry {
	+0	kind:		u8;	// what kind of log entry
	+1	size:		u8;	// number of bytes being changed (1, 2, 4, 8)
	+8	offset:		u64;	// offset of first changed byte, within the region
	+16	old_val:	u64;
	+24	new_val:	u64;
	=32;
}

// Wrapper ADT to store the cached value and its width (size)
type CachedUpdate(value: u64, size: u4) #unboxed { }

// A write-behind cache for the Write Ahead Log (WAL).
// This component buffers writes in DRAM.
//
// CONSTRAINT: ALIGNED ACCESS ONLY
// This implementation uses a HashMap to store updates. It does not support spatial
// queries or overlapping intervals. Consequently:
// 1. Reads must match the exact address of previous writes.
// 2. Mixed-width accesses (e.g., writing 8 bytes at 0x10, then reading 1 byte at 0x11)
//    will result in a cache miss, returning stale data.
class WALCache {
        private var cache: HashMap<Pointer, CachedUpdate>;

        new() {
                cache = HashMap<Pointer, CachedUpdate>.new(hash, Pointer.==);
        }

        // Check if an address is dirty (has cached changes)
        def isDirty(addr: Pointer) -> bool {
                return cache.has(addr);
        }

        // Clear the cache (re-initializes the map)
        def clear() {
                cache = HashMap<Pointer, CachedUpdate>.new(hash, Pointer.==);
        }

	// Flushes cached updates to actual memory
	def applyToMemory() {
		cache.apply(
			fun (addr: Pointer, c: CachedUpdate) {
				match (c.size) {
					1 => addr.store<u8>(u8.view(c.value));
					2 => addr.store<u16>(u16.view(c.value));
					4 => addr.store<u32>(u32.view(c.value));
					8 => addr.store<u64>(u64.view(c.value));
					_ => ;
				}
			}
		);
	}

	// Cannot have a generic read<T> because we can't cast u64 -> T generically

	def readU8(addr: Pointer) -> u8 {
		if (cache.has(addr)) return u8.view(cache[addr].value);
		return addr.load<u8>();
	}

	def readI16(addr: Pointer) -> i16 {
		if (cache.has(addr)) return i16.view(cache[addr].value);
		return addr.load<i16>();
	}

	def readU64(addr: Pointer) -> u64 {
		if (cache.has(addr)) return u64.view(cache[addr].value);
		return addr.load<u64>();
	}

	def readI64(addr: Pointer) -> i64 {
		if (cache.has(addr)) return i64.view(cache[addr].value);
		return addr.load<i64>();
	}

	// Write BlockEntry.used (u1/bool stored as u8)
	def writeU8(addr: Pointer, val: u8) {
		writeHelper(addr, 1, u64.view(val));
	}

	// Write BlockEntry.listNum
	def writeI16(addr: Pointer, val: i16) {
		writeHelper(addr, 2, u64.view(u16.view(val)));
	}

	// Write ChunkHeader.offset, limit
	def writeU64(addr: Pointer, val: u64) {
		writeHelper(addr, 8, val);
	}

	// Write BlockEntry.prev, next, listPrev, listNext
	def writeI64(addr: Pointer, val: i64) {
		writeHelper(addr, 8, u64.view(val));
	}

	private def writeHelper(addr: Pointer, size: u4, val: u64) {
		cache[addr] = CachedUpdate(val, size);
	}

        // Hash function: Convert Pointer -> i32
        private def hash(p: Pointer) => i32.view(p - Pointer.NULL);
}

component PWRegions {
	def blockSize: u64 = 256u64 * 1024u64;
	def sanitize = true;
}

layout LineMark {
	+0	mark:		u8;	// line mark info
	=1;
}

class ImmixPWRegion extends PWRegion {
	new(backend: TxnRegionBackend, numBlocks: u64, metaDataDescs: Range<Ref<MetaDataDesc>>, blockSize: u64)
		super(backend, numBlocks, metaDataDescs, blockSize) {
		lineMarks = getLineMarks();
	}
	
	// Line mark definitions
	def var lineMarks: LayoutRange<Ref<LineMark>>;
	def var numLineMarks: u64;
	
	private def getLineMarks() -> LayoutRange<Ref<LineMark>> {
		var hdr = getHeader();
		for (i < hdr.numDescs) {
			var r = descs[i];
			if (r.kind == ImmixPWRegions.ImmixLineMarkMetaKind) {
				var spaceSize = numBlocks * getBlockSize();
				numLineMarks = spaceSize / u64.!(ImmixPWRegions.ImmixLineSize);
				var lmrng = this.rng[r.offset ..+ numLineMarks * u64.!(LineMark.size)];
				return LayoutRanges.byOffset(lmrng, u64.!(ImmixPWRegions.ImmixLineSize), LineMark.size, Ref<LineMark>.of);
			}
		}
		return null;
	}
	
	def resetAllLineMarks() {
		var lineSize = u64.!(ImmixPWRegions.ImmixLineSize);
		var offset: u64 = 0;
		while (offset < numLineMarks * lineSize) {
			lineMarks[offset].mark = ImmixPWRegions.ImmixLineMarkResetValue;
			offset = offset + lineSize;
		}
	}
}

component ImmixPWRegions {
	def ImmixLineMarkMetaKind = 0x0000_0001_0000_0000u64;
	def ImmixLineMarkResetValue = 0u8;
	def ImmixLineSize = 256;  // sample - needs to be done using the metadata descriptior

	// Line mark states (matching original Immix paper)
	// Lines are either free (unmarked) or contain live objects (marked)
	def LINE_FREE: u8 = 0;    // unmarked - line is available for allocation
	def LINE_MARKED: u8 = 1;  // marked - line contains live objects
}


component MemRegions {
	def defaultMetaDataDescs = Array<byte>.new(MetaDataDesc.size * 1);
	def lineMarkDescRef = Ref<MetaDataDesc>.at(defaultMetaDataDescs, 0);
	def BlockSize = 512u64 * 1024u64;
	new() {
		var r = lineMarkDescRef;
		r.kind = ImmixPWRegions.ImmixLineMarkMetaKind;
		r.fixedBytes = 0;
		r.unitSize = u32.!(ImmixPWRegions.ImmixLineSize);
		r.bytesPerUnit = u32.!(LineMark.size);
		// leave offset unset (i.e., 0)
	}
}
