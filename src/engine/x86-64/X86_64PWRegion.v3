// Really want to write something like this, Virgil does not support it:
//
// class LayoutRange<Ref<L>> {
//	def var rng: Range<byte>;
//	new(rng);
//	def [index: u64] => Ref<L>.at(rng[index * L.size ..+ L.size]);
// }

// Layout ranges may go into a Virgil library once tested and refined

// The concept is essentially an array/range of a layout, i.e., multiple
// copies of the same layout up against each other.  The "at" function does
// the actual indexing and constructs a Ref, so this class does little actual
// work (but see the companion component, LayoutRanges).

class LayoutRange<T> {
	def var rng: Range<byte>;  // the underlying range of bytes
	def at: (Range<byte>, u64) -> T;  // a function to index that range and return a Ref
	def offset: u64 -> u64;  // a function to go from index to offset
	def [index: u64] => at(rng, index);  // indexing that supports [i] notation
	new(rng, at, offset) { }
}

// This component provides several functions that can be used to index a
// LayoutRange with different semantics for the index passed in.

component LayoutRanges {

	// atIndex interprets the index as indicating which element of the
	// range is desired.  Thus, an index of 2 indicates the third element
	// of the range.  atIndex requires the size in order to calculate the
	// byte offset.
	def atIndex<T>(rng: Range<byte>, index: u64, size: int, at: Range<byte> -> T) -> T {
		return at(rng[index * u64.!(size) ..+ size]);
	}

	// atByte interprets the index as indicating the starting byte.  Thus, an index
	// of 2 indicates to start at the third *byte* on the range.  It uses the size
	// only to determine the size of the subrange passed to "at".
	def atByte<T>(rng: Range<byte>, pos: u64, size: int, at: Range<byte> -> T) -> T {
		return at(rng[pos ..+ size]);
	}

	// forOffset takes an offset (into some other range of bytes) and
	// converts that into a suitable index in this LayoutRange based on
	// their being one layout per some number of bytes in the other range.
	// Here an offset of 2 means to convert from that offset in the
	// original range.  To perform the computation, forOffset requires a
	// "divisor" (the number of bytes in the other range per layout in
	// this range) and the size of each layout in this range.
	def forOffset<T>(rng: Range<byte>, offset: u64, divisor: u64, size: int,
			at: Range<byte> -> T) -> T {
		return at(rng[(offset / divisor) * u64.!(size) ..+ size]);
	}

	private def offset(base: u64, size: int, index: u64) -> u64 {
		return base + index * u64.!(size);
	}
	// Creates a LayoutRange that uses the atIndex accessor and gives offset as within the original range
	// Example: LayoutRanges.subrangeByIndex(rng, start, L.size, count, Ref<L>.at)
	def subrangeByIndex<T>(rng: Range<byte>, base: u64, size: int, count: u64, atFunc: Range<byte> -> T) -> LayoutRange<T> {
		var sub = rng[base ..+ count * u64.!(size)];
		return LayoutRange<T>.new(sub, LayoutRanges.atIndex(_, _, size, atFunc), offset(base, size, _));
	}

	// Creates a LayoutRange that uses the atIndex accessor
	// Example: LayoutRanges.byIndex(rng, L.size, Ref<L>.at)
	def byIndex<T>(rng: Range<byte>, size: int, atFunc: Range<byte> -> T) -> LayoutRange<T> {
		return LayoutRange<T>.new(rng, LayoutRanges.atIndex(_, _, size, atFunc), u64.*(u64.!(size),_));
	}

	// Creates a LayoutRange that uses the atByte accessor
	// Example: LayoutRanges.byByte(rng, L.size, Ref<L>.at)
	def byByte<T>(rng: Range<byte>, size: int, atFunc: Range<byte> -> T) -> LayoutRange<T> {
		return LayoutRange<T>.new(rng, LayoutRanges.atByte(_, _, size, atFunc), u64.*(u64.!(size),_));
	}

	// Creates a LayoutRange that uses the forOffset accessor
	// Example: LayoutRanges.byOffset(rng, otherUnit, L.size, Ref<L>.at)
	def byOffset<T>(rng: Range<byte>, divisor: u64, size: int, atFunc: Range<byte> -> T) -> LayoutRange<T> {
		return LayoutRange<T>.new(rng, LayoutRanges.forOffset(_, _, divisor, size, atFunc), u64.*(u64.!(size),_));
	}
}

// Don't see any advantage of Pointer based solution right now ...
/*
class LayoutPtr<T> {
	def atFunc: Range<byte> -> T;  // a function to go from bytes to T (Ref<L> for some L)
	def size: int;  // field (like this), or argument to each call?
	def atByte(pos: u64) -> T {
		// TODO: insure pos < limit
		return atFunc(CiRuntime.forgeRange<byte>(p + pos, size));
	}
	def at() -> T { return atByte(0); }
	def atIndex(index: u64) -> T { return atByte(index * size); }

	def var p: Pointer;
	def var limit: long;
	new(rng: Range<byte>, atFunc, size) {
		p = Pointer.atElement(rng, 0);
		limit = rng.size - size;
	}
}
*/

layout PWRegionHeader {
	+0	blockTable:	u64;	// relative offset to the block table
	+8	sentinels:	u64;	// offset of the sentinels (whatever type are!)
	+16	numBlocks:	u64;
	+24	numBytes:	u64;
	+32	numDescs:	u64;
	+40	metaData:	u64;	// relative offset to the metadata area
	+48	metaDataDescs:	u64;	// relative offset to the metadata descriptors
	=56;				// size of header
}

layout MetaDataDesc {
	+0	kind:		u64;  // unique code, identifying the kind of metadata
	+8	fixedBytes:	u32;  // single-copy data at beginning of metadata block
	+12	unitSize:	u32;  // number of bytes of data space per metadata item
	+16	bytesPerUnit:	u32;  // size of each metadata item, in bytes
				      // gap
	+24	offset:		u64;  // offset in region of the actual metadata
	=32;
}

type BlockEntryHandle(addr: Pointer) #unboxed {
	def isNull() -> bool {
		return addr == Pointer.NULL;
	}

	def toHandle(offset: i64) -> BlockEntryHandle {
		return BlockEntryHandle(Pointer.NULL + offset);
	} 

	def toOffset(handle: BlockEntryHandle) -> i64 {
		return this.addr - handle.addr;
	}

	def getRef() -> Ref<BlockEntry> {
		return Ref<BlockEntry>.of(CiRuntime.forgeRange<byte>(addr, BlockEntry.size));
	}

	def getListNum() -> i16 {
		return getRef().listNum;
	}

	def getUsed() -> bool {
		return getRef().used;
	}

	// // if we use relative addressing (not offsets)
	// def getPrev() -> BlockEntryHandle {
	// 	var delta = getRef().prev;
	// 	return BlockEntryHandle(if(delta == 0, Pointer.NULL, addr + i64.view(delta)));
	// }

	def getPrev() -> BlockEntryHandle {
		return toHandle(i64.view(getRef().prev));
	}

	def getNext() -> BlockEntryHandle {
		return toHandle(i64.view(getRef().next));
	}

	def getListPrev() -> BlockEntryHandle {
		return toHandle(i64.view(getRef().listPrev));
	}

	def getListNext() -> BlockEntryHandle {
		return toHandle(i64.view(getRef().listNext));
	}

	def setListNum(listNum: i16) {
		getRef().listNum = listNum;
	}

	def setUsed(used: bool) {
		getRef().used = used;
	}

	def setPrev(prev: BlockEntryHandle) {
		getRef().prev = u64.view(toOffset(prev));
	}

	def setNext(next: BlockEntryHandle) {
		getRef().next = u64.view(toOffset(next));
	}

	def setListPrev(listPrev: BlockEntryHandle) {
		getRef().listPrev = u64.view(toOffset(listPrev));
	}

	def setListNext(listNext: BlockEntryHandle) {
		getRef().listNext = u64.view(toOffset(listNext));
	}
}

def getBlockEntryHandle(r: Ref<BlockEntry>) -> BlockEntryHandle {
	return BlockEntryHandle(Pointer.atRef(r));
}


// Unsure if needed or even correct
type ChunkHeaderHandle(region: X86_64PWRegion, addr: Pointer) {
	def getRef() -> Ref<ChunkHeader> {
		return Ref<ChunkHeader>.of(CiRuntime.forgeRange<byte>(addr, ChunkHeader.size));
	}
 }

layout BlockEntry {
	+0	listNum:	i16;  // which list is this entry part of
	+2	used:		bool; // is this block in use?
				      // gap
	+8	prev:		u64;  // prev entry in memory;
	+16	next:		u64;  // next entry in memory order
	+24	listPrev:	u64;  // prev entry on same list
	+32	listNext:	u64;  // next entry on same list
	=40;			      // size of block entry
}

layout ChunkHeader {
	+0	offset:		u64; // offset in chunk for next allocation
	+8	limit:		u64; // offset just beyond last available byte in chunk
	+16	chunk_limit:	u64; // offset just beyond end of the chunk
	+24	linemarks:	u64; // offset to first link mark of the chunk
	=32;			     // size of chunk header
}

enum ListKind(code: i16) {
	METADATA(-1),
	NONE(-2),
	USED(-3),
	SMALL_FREE(0),
	LARGE_FREE(1)
}

component BlockLists {
	def MAX_LISTS = 8;	// maximum number of lists == number of list sentinels
				// METADATA, etc., do not have sentinels and not part of this count
}

class X86_64PWRegion {
	def var mapping: Mapping;
	def var rng: Range<byte>;  // the actual bytes of the region
	def numBlocks: u64;
	def metaDataDescs: Range<Ref<MetaDataDesc>>;
	def var descs: LayoutRange<Ref<MetaDataDesc>>;
	def var sentinels: LayoutRange<Ref<BlockEntry>>;
	def var smallSentinel: BlockEntryHandle;
	def var largeSentinel: BlockEntryHandle;
	def blankChunkHeader = Array<byte>.new(ChunkHeader.size);  // so we can return *something* when allocation fails
	def blankChunkHeaderRef = Ref<ChunkHeader>.of(blankChunkHeader);
	var blockLayouts: LayoutRange<Ref<BlockEntry>>;

	new(numBlocks, metaDataDescs) { }
	private def sanitizeOffset(offset: u64) {
		if (PWRegions.sanitize) {
			var addr: long = Pointer.atElement(rng, 0) - Pointer.NULL;
			if (offset < rng.length) {
				Trace.OUT.put3("offset %d is good for region at 0x%x of length %d", offset, addr, rng.length).ln();
			} else {
				Trace.OUT.put3("offset %d is bad for region at 0x%x of length %d", offset, addr, rng.length).ln();
			}
		}
	}
	private def sanitizeAddress(addr: Pointer) {
		if (PWRegions.sanitize) {
			var delta: long = addr - Pointer.atElement(rng, 0);
			var addrAsLong: long = addr - Pointer.NULL;
			if (delta < 0 || delta >= rng.length) {
				Trace.OUT.put3("address 0x%x is bad for region at 0x%x of length %d", addr, addrAsLong, rng.length).ln();
			} else {
				Trace.OUT.put3("address 0x%x is good for region at 0x%x of length %d", addr, addrAsLong, rng.length).ln();
			}
		}
	}
	private def init() {
		def nullBTE = BlockEntryHandle(Pointer.NULL);
		var size = numBlocks * PWRegions.blockSize;
		var hdr = getHeader();
		sanitizeAddress(Pointer.atRef(hdr));
		hdr.numBlocks = numBlocks;
		hdr.numBytes = size;
		var currentRegionEnd = size;

		// set up list sentinels just after the header
		hdr.sentinels = u64.!(PWRegionHeader.size);
		sentinels = LayoutRanges.subrangeByIndex<Ref<BlockEntry>>(
					rng, hdr.sentinels, BlockEntry.size, u64.!(BlockLists.MAX_LISTS), Ref<BlockEntry>.of);
		sanitizeOffset(hdr.sentinels);
		sanitizeOffset(hdr.sentinels + u64.!(sentinels.rng.length));
		for (i < BlockLists.MAX_LISTS) {
			var entryRef = sentinels[u64.!(i)];
			var entryHandle = getBlockEntryHandle(entryRef);
			entryHandle.setListNum(0);
			entryHandle.setUsed(false);
			entryHandle.setPrev(nullBTE);
			entryHandle.setNext(nullBTE);
			entryHandle.setListPrev(nullBTE);
			entryHandle.setListNext(nullBTE);
		}

		// take space for block table
		var blockTableSize = (numBlocks + 1) * u64.!(BlockEntry.size);
		currentRegionEnd -= blockTableSize;
		hdr.blockTable = currentRegionEnd;
		sanitizeOffset(currentRegionEnd);
		blockLayouts = LayoutRanges.byIndex<Ref<BlockEntry>>(
					rng[hdr.blockTable ..+ blockTableSize], BlockEntry.size, Ref<BlockEntry>.of);

		// clear block table entries (filled in better later)
		for (i < numBlocks + 1) {
			var entryRef = blockLayouts[i];
			var entryHandle = getBlockEntryHandle(entryRef);
			entryHandle.setListNum(0);
			entryHandle.setUsed(false);
			entryHandle.setPrev(nullBTE);
			entryHandle.setNext(nullBTE);
			entryHandle.setListPrev(nullBTE);
			entryHandle.setListNext(nullBTE);
		}

		// take space for metadata descriptors
		var metaDataDescsSize = metaDataDescs.length * MetaDataDesc.size;
		currentRegionEnd -= u64.!(metaDataDescsSize);
		sanitizeOffset(currentRegionEnd);
		hdr.metaDataDescs = currentRegionEnd;
		hdr.numDescs = u64.!(metaDataDescs.length);
		var metaDataLayouts = LayoutRanges.byIndex<Ref<MetaDataDesc>>(
					rng[hdr.metaDataDescs ..+ metaDataDescsSize], MetaDataDesc.size, Ref<MetaDataDesc>.of);

		// fill metadata descriptors and allocate metadata space
		for (i < metaDataDescs.length) {
			var desc = metaDataDescs[i];
			var metaDataSize = u64.!(desc.fixedBytes);
			var numUnits = (size + desc.unitSize - 1) / desc.unitSize;
			metaDataSize += numUnits * desc.bytesPerUnit;
			currentRegionEnd -= metaDataSize;
			sanitizeOffset(currentRegionEnd);
			var newDesc = metaDataLayouts[u64.!(i)];
			newDesc.kind = desc.kind;
			newDesc.fixedBytes = desc.fixedBytes;
			newDesc.unitSize = desc.unitSize;
			newDesc.bytesPerUnit = desc.bytesPerUnit;
			newDesc.offset = currentRegionEnd;
		}

		var metaDataBytes = u64.!(rng.length) - currentRegionEnd;
		var metaDataBlocks = (metaDataBytes + (PWRegions.blockSize - 1)) / PWRegions.blockSize;
		var userDataBlocks = numBlocks - (metaDataBlocks + 1);
		

		// set up block lists
		var firstRef = blockLayouts[0];
		var firstHandle = getBlockEntryHandle(firstRef);

		var secondRef = blockLayouts[1];
		var secondHandle = getBlockEntryHandle(secondRef);
	
		var smallSentinelRef = sentinels[u64.!(ListKind.SMALL_FREE.code)];
		smallSentinel = getBlockEntryHandle(smallSentinelRef);

		var largeSentinelRef = sentinels[u64.!(ListKind.LARGE_FREE.code)];
		largeSentinel = getBlockEntryHandle(largeSentinelRef);

		var metaRef = blockLayouts[userDataBlocks + 1];
		var metaHandle = getBlockEntryHandle(metaRef);

		var lastRef = blockLayouts[numBlocks];
		var lastHandle = getBlockEntryHandle(lastRef);

		sanitizeAddress(firstHandle.addr);
		firstHandle.setListNum(ListKind.METADATA.code);
		firstHandle.setUsed(true);
		firstHandle.setNext(secondHandle);
		firstHandle.setListPrev(smallSentinel);
		sanitizeOffset(blockLayouts.offset(1));

		sanitizeAddress(secondHandle.addr);
		secondHandle.setListNum(ListKind.LARGE_FREE.code);
		secondHandle.setNext(metaHandle);
		secondHandle.setPrev(firstHandle);
		secondHandle.setListPrev(largeSentinel);  // TODO
		sanitizeOffset(blockLayouts.offset(userDataBlocks + 1));
		sanitizeOffset(blockLayouts.offset(0));
		sanitizeOffset(sentinels.offset(u64.!(ListKind.LARGE_FREE.code)));

		sanitizeAddress(smallSentinel.addr);
		smallSentinel.setListNext(firstHandle);
		sanitizeOffset(blockLayouts.offset(0));

		sanitizeAddress(largeSentinel.addr);
		largeSentinel.setListNext(secondHandle);
		sanitizeOffset(blockLayouts.offset(1));
		
		sanitizeAddress(metaHandle.addr);
		metaHandle.setListNum(ListKind.METADATA.code);
		metaHandle.setUsed(true);
		metaHandle.setNext(lastHandle);
		metaHandle.setPrev(secondHandle);
		sanitizeOffset(blockLayouts.offset(numBlocks));
		sanitizeOffset(blockLayouts.offset(1));
		
		sanitizeAddress(lastHandle.addr);
		lastHandle.setListNum(ListKind.NONE.code);
		lastHandle.setUsed(true);
		lastHandle.setPrev(metaHandle);
		sanitizeOffset(blockLayouts.offset(userDataBlocks + 1));
	}
	def getHeader() -> Ref<PWRegionHeader> {
		return Ref<PWRegionHeader>.of(mapping.range.range(0, PWRegionHeader.size));
	}
	def regionHeaderSize() -> int {
		return PWRegionHeader.size;  // default implementation
	}
	def getBlockSize() -> u64;
	def defaultMetadataDescs() -> LayoutRange<Ref<MetaDataDesc>>;
	def metadataDescs() -> LayoutRange<Ref<MetaDataDesc>> {
		var hdr = getHeader();
		var metaDataDescsSize = hdr.numDescs * u64.!(MetaDataDesc.size);
		return LayoutRanges.byIndex<Ref<MetaDataDesc>>(
					rng[hdr.metaDataDescs ..+ metaDataDescsSize], MetaDataDesc.size, Ref<MetaDataDesc>.of);
	}
	def blockIndexOfEntry(entry: BlockEntryHandle) -> u64 {
		var blkTablePtr = Pointer.atElement(rng, 0) + i64.!(getHeader().blockTable);
		var entryPtr = entry.addr;
		return u64.!((entryPtr - blkTablePtr) / BlockEntry.size);
	}
	def blockOffsetOfIndex(index: u64) -> u64 {
		return index * getBlockSize();
	}
	def createChunk(startEntry: BlockEntryHandle, numBlocks: int) -> Ref<ChunkHeader> {
		var blockOffset = blockOffsetOfIndex(blockIndexOfEntry(startEntry));
		var chunk = Ref<ChunkHeader>.of(rng[blockOffset ..+ ChunkHeader.size]);
		chunk.offset = u64.!(ChunkHeader.size);
		chunk.limit = u64.!(numBlocks) * getBlockSize();
		// TODO link mark field
		return chunk;
	}
	def allocChunk(numBlocks: int) -> Ref<ChunkHeader> {
		if (numBlocks == 1) {
			// Try the small free block list
			var firstHandle = smallSentinel.getListNext();
			if (!firstHandle.isNull()) {
				removeFromList(firstHandle);
				firstHandle.setUsed(true);
				return createChunk(firstHandle, 1);
			}
		}
		var thisHandle = largeSentinel.getListNext();
		while (!thisHandle.isNull()) {
			var thisIndex = blockIndexOfEntry(thisHandle);
			var nextHandle = thisHandle.getNext();
			if (nextHandle.isNull()) return blankChunkHeaderRef;
			var nextIndex = blockIndexOfEntry(nextHandle);
			var chunkBlocks = nextIndex - thisIndex;
			if (numBlocks > chunkBlocks) {
				thisHandle = thisHandle.getNext();
				continue;
			}
			if (numBlocks == chunkBlocks) {
				removeFromList(thisHandle);
				thisHandle.setUsed(true);
				return createChunk(thisHandle, numBlocks);
			}
			// need to split the block
			var remIndex = thisIndex + u64.!(numBlocks);
			var remRef = blockLayouts[remIndex];
			var remHandle = getBlockEntryHandle(remRef);
			var remBlocks = nextIndex - remIndex;

			remHandle.setListNum(if(remBlocks == 1, ListKind.SMALL_FREE.code, ListKind.LARGE_FREE.code));
			remHandle.setUsed(false);
			remHandle.setPrev(thisHandle);
			remHandle.setNext(nextHandle);
			remHandle.setListPrev(thisHandle);
			remHandle.setListNext(nextHandle);
			thisHandle.setNext(remHandle);
			thisHandle.setListNext(remHandle);
			nextHandle.setPrev(remHandle);
			var listNextHandle = thisHandle.getListNext();
			listNextHandle.setListPrev(remHandle);
			removeFromList(thisHandle);
			return createChunk(thisHandle, numBlocks);
		}
		return blankChunkHeaderRef;
	}
/*	// Alternative using BlockEntryHandle, etc., and relative offsets
	def allocChunk(numBlocks: int) -> ChunkHeaderHandle {
		if (numBlocks == 1) {
			// Try the small free block list
			var firstBlock = BlockEntryHandle(Pointer.NULL + long.!(smallSentinel.listNext));
			if (firstBlock.addr != Pointer.NULL) {
				// var firstRef = blockEntryAt(firstBlock);
				removeFromList(firstBlock.getRef());
				firstBlock.setUsed(true);
				return createChunk(firstBlock.getRef(), 1);
			}
		}
/*
		var thisBlock = largeSentinel.listNext;
		while (thisBlock.addr != Pointer.NULL) {
			var thisIndex = thisBlock.getIndex();
			var next = thisBlock.getNext();
			if (next.addr == Pointer.NULL) return blankChunkHeaderHandle;
			var nextIndex = next.getIndex();
			var chunkBlocks = nextIndex - thisIndex;
			if (numBlocks > chunkBlocks) {
				thisBlock = next;
				continue;
			}
			// ...
			if (numBlocks == chunkBlocks) {
				removeFromList(thisRef);
				thisRef.used = true;
				return createChunk(thisRef, numBlocks);
			}
			// need to split the block
			var remIndex = thisIndex + u64.!(numBlocks);
			var remRef = blockLayouts[remIndex];
			var remBlocks = nextIndex - remIndex;
			var remOffset = blockOffsetOfIndex(remIndex);
			remRef.listNum = if(remBlocks == 1, ListKind.SMALL_FREE.code, ListKind.LARGE_FREE.code);
			remRef.used = false;
			remRef.prev = thisBlock;
			remRef.next = next;
			remRef.listPrev = thisBlock;
			remRef.listNext = thisRef.next;
			thisRef.next = remOffset;
			thisRef.listNext = remOffset;
			nextRef.prev = remOffset;
			var listNextRef = blockEntryAt(thisRef.listNext);
			listNextRef.listPrev = remOffset;
			removeFromList(thisRef);
			return createChunk(thisRef, numBlocks);
		}
		return blankChunkHeaderRef;
	}
*/
	def blockEntryAt(offset: u64) -> Ref<BlockEntry> {
		return Ref<BlockEntry>.of(rng[offset ..+ BlockEntry.size]);
	}
	def removeFromList(blk: BlockEntryHandle) {
		var prev = blk.getListPrev();
		var next = blk.getListNext();
		if (!prev.isNull()) {
			prev.setListNext(next);
		}
		if (!next.isNull()) {
			next.setListPrev(prev);
		}
		def nullBTE = BlockEntryHandle(Pointer.NULL);
		blk.setListPrev(nullBTE);
		blk.setListNext(nullBTE);
		blk.setListNum(ListKind.NONE.code);
	}
	// ...
}

component PWRegions {
	def blockSize: u64 = 256u64 * 1024u64;
	def sanitize = true;
}

layout LineMark {
	+0	mark:		u8;	// line mark info
	=1;
}

class ImmixPWRegion extends X86_64PWRegion {
	new(numBlocks: u64, metaDataDescs: Range<Ref<MetaDataDesc>>)
		super(numBlocks, metaDataDescs) { }
	private def init() {
		X86_64PWRegion.init(this);
		lineMarks = getLineMarks();
	}
	// Line mark definitions
	def var lineMarks: LayoutRange<Ref<LineMark>>;
	def var numLineMarks: u64;
	private def getLineMarks() -> LayoutRange<Ref<LineMark>> {
		var hdr = getHeader();
		for (i < hdr.numDescs) {
			var r = descs[i];
			if (r.kind == ImmixPWRegions.ImmixLineMarkMetaKind) {
				var spaceSize = numBlocks * getBlockSize();
				numLineMarks = spaceSize / u64.!(ImmixPWRegions.ImmixLineSize);
				var lmrng = this.rng[r.offset ..+ numLineMarks * u64.!(LineMark.size)];
				return LayoutRanges.byOffset(lmrng, u64.!(ImmixPWRegions.ImmixLineSize), LineMark.size, Ref<LineMark>.of);
			}
		}
		return null;
	}
	def resetAllLineMarks() {
		for (i < numLineMarks) {
			lineMarks[i].mark = ImmixPWRegions.ImmixLineMarkResetValue;
		}
	}
}

component ImmixPWRegions {
	def ImmixLineMarkMetaKind = 0x0000_0001_0000_0000u64;
	def ImmixLineMarkResetValue = 0u8;
	def ImmixLineSize = 256;  // sample - needs to be done using the metadata descriptior
}


component MemRegions {
	def defaultMetaDataDescs = Array<byte>.new(MetaDataDesc.size * 1);
	def lineMarkDescRef = Ref<MetaDataDesc>.at(defaultMetaDataDescs, 0);
	def BlockSize = 512u64 * 1024u64;
	new() {
		var r = lineMarkDescRef;
		r.kind = ImmixPWRegions.ImmixLineMarkMetaKind;
		r.fixedBytes = 0;
		r.unitSize = u32.!(ImmixPWRegions.ImmixLineSize);
		r.bytesPerUnit = u32.!(LineMark.size);
		// leave offset unset (i.e., 0)
	}
}

class X86_64PWMemRegion extends X86_64PWRegion {
	new(numBlocks: u64, metaDataDescs: Range<Ref<MetaDataDesc>>) super(numBlocks, metaDataDescs) {
		var size = numBlocks * MemRegions.BlockSize;
		var prot = Mmap.PROT_READ | Mmap.PROT_WRITE;
		var mapping = Mmap.reserve(size, prot);
		if (mapping == null) {
			// failure
		}
		init();
	}
	def getBlockSize() -> u64 { return MemRegions.BlockSize; }
}

class X86_64PWNVRegion extends X86_64PWRegion {
	// ...
	new (path: string, numBlocks: u64, metaDataDescs: Range<Ref<MetaDataDesc>>) super(numBlocks, metaDataDescs) {
		// ...
		init();
	}
}
